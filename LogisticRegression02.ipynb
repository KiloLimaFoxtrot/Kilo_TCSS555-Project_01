{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "LogisticRegression.ipynb",
   "provenance": [],
   "authorship_tag": "ABX9TyN6Tj1R2lPK6DEWT+riPtt/"
  },
  "kernelspec": {
   "name": "pycharm-1fa8f89d",
   "language": "python",
   "display_name": "PyCharm (TCSS_555_A_Machine_Learning)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9oOtPVfD6VW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1607912704766,
     "user_tz": 360,
     "elapsed": 1214,
     "user": {
      "displayName": "Mike Cresswell",
      "photoUrl": "",
      "userId": "05784128764127520699"
     }
    },
    "outputId": "8d626986-e7c2-45e1-8017-2d7a896be56e"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import io\n",
    "import requests\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Get Processed Data\n",
    "url=\"https://raw.githubusercontent.com/mgcresswell/TCSS555-Project/main/deceptive-opinion_processed.csv\"\n",
    "s=requests.get(url).content\n",
    "Corpus = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "y = Corpus['deceptive']\n",
    "X = Corpus.drop(['id','deceptive'], axis=1)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "hotelEncoded = label_encoder.fit_transform(X['hotel'])\n",
    "polarityEncoded = label_encoder.fit_transform(X['polarity'])\n",
    "sourceEncoded = label_encoder.fit_transform(X['source'])\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "hotelEncoded = hotelEncoded.reshape(len(hotelEncoded), 1)\n",
    "X['hotel'] = onehot_encoder.fit_transform(hotelEncoded)\n",
    "polarityEncoded = polarityEncoded.reshape(len(polarityEncoded), 1)\n",
    "X['polarity'] = onehot_encoder.fit_transform(polarityEncoded)\n",
    "sourceEncoded = sourceEncoded.reshape(len(sourceEncoded), 1)\n",
    "X['source'] = onehot_encoder.fit_transform(sourceEncoded)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=3500)\n",
    "Tfidf_vect.fit(Corpus['text'])\n",
    "X_Tfidf = Tfidf_vect.transform(X['text'])\n",
    "X['text'] = X_Tfidf.toarray()\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, test_index in split.split(X, y):\n",
    "   Train_X, Test_X = X.loc[train_index], X.loc[test_index]\n",
    "   Train_Y, Test_Y = y[train_index], y[test_index]\n",
    "\n",
    "model = LogisticRegression(C=1.0).fit(Train_X, Train_Y)\n",
    "start = time.perf_counter()\n",
    "model.fit(Train_X, Train_Y)\n",
    "stop = time.perf_counter()\n",
    "\n",
    "#Test Accuracy \n",
    "y_pred = model.predict(Test_X)\n",
    "test_Accuracy = accuracy_score(Test_Y, y_pred)*100\n",
    "\n",
    "#Test Accuracy \n",
    "y_pred = model.predict(Train_X)\n",
    "train_Accuracy = accuracy_score(Train_Y, y_pred)*100\n",
    "\n",
    "crossvalMean = cross_val_score(model, X, y, cv=10).mean()\n",
    "\n",
    "curTime = stop - start;\n",
    "print(f\"Training Time = {curTime:0.8f} Seconds\")\n",
    "print(f\"Average Accuracy = {test_Accuracy}\")\n",
    "print(f\"Average Training Time = {train_Accuracy}\")\n",
    "print(f\"Cross Validation Mean = {crossvalMean}\")\n",
    "\n"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Training Time = 0.00554361 Seconds\n",
      "Average Accuracy = 100.0\n",
      "Average Training Time = 100.0\n",
      "Cross Validation Mean = 1.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVrqUXetEORG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1607913108767,
     "user_tz": 360,
     "elapsed": 8168,
     "user": {
      "displayName": "Mike Cresswell",
      "photoUrl": "",
      "userId": "05784128764127520699"
     }
    },
    "outputId": "0b7c2dc4-c3f2-4dc3-d45a-06e8e1c31b46"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/mgcresswell/TCSS555-Project/main/deceptive-opinion_processed.csv\"\n",
    "s=requests.get(url).content\n",
    "Corpus = pd.read_csv(io.StringIO(s.decode('utf-8'))) \n",
    "\n",
    "url=\"https://raw.githubusercontent.com/mgcresswell/TCSS555-Project/main/deceptive-opinion.csv\"\n",
    "s=requests.get(url).content\n",
    "raw = pd.read_csv(io.StringIO(s.decode('utf-8'))) \n",
    "\n",
    "y = Corpus['deceptive']\n",
    "X = Corpus.drop(['id','deceptive','source'], axis=1)\n",
    "\n",
    "#feature engineering\n",
    "punc = ['`','~','!','(',')','_','-','{','[','}','}',':',';','\"',',','.','?','/','\"\"']\n",
    "X['char_count'] = raw[\"text\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
    "X['total_length'] = raw['text'].apply(len)\n",
    "X['punc_count'] = raw['text'].apply(lambda x : len([a for a in x if a in punc]))\n",
    "X['word_count'] = raw[\"text\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "X['char_count'] = raw[\"text\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
    "X['sentence_count'] = raw[\"text\"].apply(lambda x: len(str(x).split(\".\")))\n",
    "X['avg_word_length'] = X['char_count'] / X['word_count']\n",
    "X['avg_sentence_length'] = X['word_count'] / X['sentence_count']\n",
    "X['word_density'] = X['word_count'] / (X['char_count'] + 1)\n",
    "X['punc_count'] = raw['text'].apply(lambda x : len([a for a in x if a in punc]))\n",
    "X['total_length'] = raw['text'].apply(len)\n",
    "X['capitals'] = raw['text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "X['num_exclamation_marks'] = raw['text'].apply(lambda x: x.count('!'))\n",
    "X['num_question_marks'] = raw['text'].apply(lambda x: x.count('?'))\n",
    "X['num_punctuation'] = raw['text'].apply(lambda x: sum(x.count(w) for w in '.,;:'))\n",
    "X['num_symbols'] = raw['text'].apply(lambda x: sum(x.count(w) for w in '*&$%'))\n",
    "X['num_unique_words'] = raw['text'].apply(lambda x: len(set(w for w in x.split())))\n",
    "X['words_vs_unique'] = X['num_unique_words'] / X['word_count']\n",
    "X[\"word_unique_percent\"] =  X[\"num_unique_words\"]*100/X['word_count']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "hotelEncoded = label_encoder.fit_transform(X['hotel'])\n",
    "polarityEncoded = label_encoder.fit_transform(X['polarity'])\n",
    "#sourceEncoded = label_encoder.fit_transform(X['source'])\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "hotelEncoded = hotelEncoded.reshape(len(hotelEncoded), 1)\n",
    "X['hotel'] = onehot_encoder.fit_transform(hotelEncoded)\n",
    "polarityEncoded = polarityEncoded.reshape(len(polarityEncoded), 1)\n",
    "X['polarity'] = onehot_encoder.fit_transform(polarityEncoded)\n",
    "#sourceEncoded = sourceEncoded.reshape(len(sourceEncoded), 1)\n",
    "#X['source'] = onehot_encoder.fit_transform(sourceEncoded)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=3500)\n",
    "Tfidf_vect.fit(Corpus['text'])\n",
    "X_Tfidf = Tfidf_vect.transform(X['text'])\n",
    "X['text'] = X_Tfidf.toarray()\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, test_index in split.split(X, y):\n",
    "   Train_X, Test_X = X.loc[train_index], X.loc[test_index]\n",
    "   Train_Y, Test_Y = y[train_index], y[test_index]\n",
    "\n",
    "model = LogisticRegression(C=1.0, max_iter=10000).fit(Train_X, Train_Y)\n",
    "start = time.perf_counter()\n",
    "model.fit(Train_X, Train_Y)\n",
    "stop = time.perf_counter()\n",
    "\n",
    "#Test Accuracy \n",
    "y_pred = model.predict(Test_X)\n",
    "test_Accuracy = accuracy_score(Test_Y, y_pred)*100\n",
    "\n",
    "#Test Accuracy \n",
    "y_pred = model.predict(Train_X)\n",
    "train_Accuracy = accuracy_score(Train_Y, y_pred)*100\n",
    "\n",
    "crossvalMean = cross_val_score(model, X, y, cv=10).mean()\n",
    "\n",
    "curTime = stop - start;\n",
    "print(f\"Training Time = {curTime:0.8f} Seconds\")\n",
    "print(f\"Test Accuracy = {test_Accuracy}\")\n",
    "print(f\"Training Accuracy = {train_Accuracy}\")\n",
    "print(f\"Cross Validation Mean = {crossvalMean}\")"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Training Time = 0.39805280 Seconds\n",
      "Test Accuracy = 70.41666666666667\n",
      "Training Accuracy = 70.08928571428571\n",
      "Cross Validation Mean = 0.69625\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}